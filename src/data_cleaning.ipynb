import gzip
import json
import pandas as pd
from joblib import Parallel, delayed
import itertools


# Function to parse a single JSON line
def parse_json_line(line):
    line = line.strip()
    if line:  # Skip empty lines
        try:
            return json.loads(line)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            return None  # Skip malformed lines
    return None

# Function to read and process the .json.gz file using joblib for parallelism
def read_json_gz_parallel_joblib(file_path, n_jobs=4):
    with gzip.open(file_path, 'rt', encoding='utf-8') as f:
        lines = f.readlines()  # Read all lines first

    # Use joblib's Parallel to process the lines in parallel
    results = Parallel(n_jobs=n_jobs)(delayed(parse_json_line)(line) for line in lines)
    
    # Filter out None values (invalid or empty lines)
    data = [result for result in results if result is not None]
    
    return data

# Usage
file_path = "your file path"

data = read_json_gz_parallel_joblib(file_path, n_jobs=24)  # Adjust n_jobs based on your system

# Now 'data' contains the JSON objects from the file
print(data[:10])

def flatten_data(json_data):
    rows = []
    for entry in json_data:
        for transcript_id, positions in entry.items():
            for position, sequences in positions.items():
                for sequence_key, sequence_data in sequences.items():
                    rows.append({
                        'transcript_id': transcript_id,
                        'transcript_position': int(position),  # Convert to int to match `info_data`
                        'sequence_key': sequence_key,
                        'sequence_data': sequence_data
                    })
    return pd.DataFrame(rows)

# Flatten the JSON data
data_flattened = flatten_data(data)

print(data_flattened.head())

def process_row(row):
    transcript_id = row['transcript_id']
    transcript_position = row['transcript_position']
    sequence_key = row['sequence_key']
    
    # Converting string representation of list to an actual list, if needed
    sequence_data = row['sequence_data']
    if isinstance(sequence_data, str):
        sequence_data = eval(sequence_data)  # Convert to list if it's a string
    
    # Define positions for -1, 0, and +1, and extract features accordingly
    row_data = {
        'transcript_id': transcript_id,
        'transcript_position': transcript_position,
        'sequence_key': sequence_key,
        'sequence_-1': sequence_key[:-2],  # Sequence for -1 position
        'sequence_0': sequence_key[1:-1],    # Sequence for 0 (central) position
        'sequence_+1': sequence_key[2:],   # Sequence for +1 position
        'features_-1': [seq[:3] for seq in sequence_data],  # Features for -1 position
        'features_0': [seq[3:6] for seq in sequence_data],  # Features for 0 position
        'features_+1': [seq[6:] for seq in sequence_data]   # Features for +1 position
    }
    
    return row_data

# Apply this processing to all rows
processed_data = pd.DataFrame([process_row(row) for index, row in data_flattened.iterrows()])

processed_data.head()

# Read the .info file as a CSV
def read_info_file(file_path):
    df = pd.read_csv(file_path)
    return df

# Usage
file_path = "C:\\Users\\ansel\\Downloads\\data.info.labelled"
info_data = read_info_file(file_path)

# Now, merge `data_flattened` with `info_data`, drop 'gene_id' column
merged_data = pd.merge(info_data, processed_data, on=['transcript_id', 'transcript_position'], how='right').drop('gene_id', axis=1)
# Display the first few rows of the merged result
print(merged_data.head())

def separate_features_for_corresponding_rows(row):
    """Separate corresponding feature lists for -1, 0, and +1 positions into individual rows."""
    transcript_id = row['transcript_id']
    transcript_position = row['transcript_position']
    label = row['label']
    sequence_key = row['sequence_key']
    
    # Extract the lists of features for each position (-1, 0, +1)
    features_minus_1 = row['features_-1']
    features_0 = row['features_0']
    features_plus_1 = row['features_+1']
    
    # The number of corresponding feature sets (assuming all positions have the same number of features)
    num_features = len(features_minus_1)
    
    # Create rows where each list of features corresponds across the -1, 0, and +1 positions
    rows = []
    for i in range(num_features):
        rows.append({
            'transcript_id': transcript_id,
            'transcript_position': transcript_position,
            'label': label,
            'sequence_key': sequence_key,
            'sequence_-1': row['sequence_-1'],
            'sequence_0': row['sequence_0'],
            'sequence_+1': row['sequence_+1'],
            # Extracting specific terms from the features lists
            'dwell_time_-1': features_minus_1[i][0],  # First term in -1 position is dwell_time
            'sd_-1': features_minus_1[i][1],          # Second term in -1 position is standard deviation (sd)
            'mean_-1': features_minus_1[i][2],        # Third term in -1 position is mean
            
            'dwell_time_0': features_0[i][0],         # First term in 0 position is dwell_time
            'sd_0': features_0[i][1],                 # Second term in 0 position is standard deviation (sd)
            'mean_0': features_0[i][2],               # Third term in 0 position is mean
            
            'dwell_time_+1': features_plus_1[i][0],   # First term in +1 position is dwell_time
            'sd_+1': features_plus_1[i][1],           # Second term in +1 position is standard deviation (sd)
            'mean_+1': features_plus_1[i][2]          # Third term in +1 position is mean
        })
    
    return rows

# Apply this processing to all rows and flatten the result
separated_data = pd.DataFrame([new_row for index, row in merged_data.iterrows() for new_row in separate_features_for_corresponding_rows(row)])

# List of numeric columns to average
numeric_columns = ['dwell_time_-1', 'sd_-1', 'mean_-1', 'dwell_time_0', 'sd_0', 'mean_0', 
                   'dwell_time_+1', 'sd_+1', 'mean_+1']

# Group by transcript_id and transcript_position, averaging only the numeric columns
averaged_numeric_data = separated_data.groupby(['transcript_id', 'transcript_position'])[numeric_columns].mean().reset_index()

# Now keep the non-numeric columns by using the first occurrence within each group
non_numeric_columns = ['sequence_key', 'sequence_-1', 'sequence_0', 'sequence_+1', 'label']  # add other columns as needed
first_non_numeric_data = separated_data.groupby(['transcript_id', 'transcript_position'])[non_numeric_columns].first().reset_index()

# Merge the averaged numeric data and the non-numeric data back together
averaged_data = pd.merge(averaged_numeric_data, first_non_numeric_data, on=['transcript_id', 'transcript_position'])

# Define the correct order of columns
column_order = [
    'transcript_id', 'transcript_position', 'label', 
    'sequence_key', 'sequence_-1', 'sequence_0', 'sequence_+1',
    'dwell_time_-1', 'sd_-1', 'mean_-1', 
    'dwell_time_0', 'sd_0', 'mean_0',
    'dwell_time_+1', 'sd_+1', 'mean_+1'
]

# Reorder the columns in averaged_data
averaged_data = averaged_data[column_order]

# Save the reordered data to a CSV file
averaged_data.to_csv("your path to stored folder\\processed_data3_averaged.csv", index=False)
